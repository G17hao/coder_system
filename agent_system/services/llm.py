"""Anthropic API å°è£… â€” å« token è®¡æ•°ã€è¶…æ—¶ã€é‡è¯•ã€æµå¼è¾“å‡º"""

from __future__ import annotations

import json
import logging
import re
import socket
import sys
import time
from dataclasses import dataclass, field
from typing import Any, TYPE_CHECKING

import anthropic

if TYPE_CHECKING:
    from agent_system.services.conversation_logger import ConversationLog

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """LLM è°ƒç”¨ç»“æœ"""
    content: str
    tool_calls: list[dict[str, Any]] = field(default_factory=list)
    input_tokens: int = 0
    output_tokens: int = 0
    stop_reason: str = ""


@dataclass
class TokenUsage:
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    total_input: int = 0
    total_output: int = 0
    total_calls: int = 0

    @property
    def total(self) -> int:
        return self.total_input + self.total_output


# åŒ¹é… <think>...</think> æ ‡ç­¾ï¼ˆå«è·¨è¡Œï¼‰ï¼Œç”¨äºè¿‡æ»¤æ¨¡å‹æ€è€ƒå†…å®¹
_THINK_RE = re.compile(r"<think>[\s\S]*?</think>", re.DOTALL)
# åŒ¹é…æœªé—­åˆçš„ <think>... ç‰‡æ®µï¼ˆæµå¼åœºæ™¯ä¸­æœ€åä¸€å—å¯èƒ½æœªå…³é—­ï¼‰
_THINK_OPEN_RE = re.compile(r"<think>[\s\S]*$", re.DOTALL)


def _strip_think_tags(text: str | None) -> str:
    """ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹"""
    if text is None:
        return ""
    if not isinstance(text, str):
        text = str(text)
    text = _THINK_RE.sub("", text)
    text = _THINK_OPEN_RE.sub("", text)
    return text.strip()


def _is_retryable_timeout_error(error: Exception) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„åº•å±‚è¶…æ—¶å¼‚å¸¸"""
    if isinstance(error, (TimeoutError, socket.timeout)):
        return True
    message = str(error).lower()
    return "timed out" in message or "timeout" in message


def _extract_api_status_error_detail(error: anthropic.APIStatusError) -> str:
    """æå– APIStatusError å¯è¯»è¯¦æƒ…ï¼ˆrequest_id/å“åº”ä½“ï¼‰"""
    parts: list[str] = []

    request_id = getattr(error, "request_id", None)
    if request_id:
        parts.append(f"request_id={request_id}")

    body = getattr(error, "body", None)
    if body:
        parts.append(f"body={body}")

    response = getattr(error, "response", None)
    if response is not None:
        headers = getattr(response, "headers", None)
        if headers is not None:
            header_request_id = headers.get("request-id") or headers.get("x-request-id")
            if header_request_id and not request_id:
                parts.append(f"request_id={header_request_id}")

        response_text = getattr(response, "text", None)
        if response_text:
            parts.append(f"response={response_text}")
        else:
            try:
                response_json = response.json()
                parts.append(f"response={response_json}")
            except Exception:
                pass

    if not parts:
        parts.append(str(error))

    detail = " | ".join(parts)
    max_len = 800
    if len(detail) > max_len:
        return detail[:max_len] + "..."
    return detail


def _estimate_request_payload(
    system_prompt: str,
    messages: list[dict[str, Any]],
    tools: list[dict[str, Any]] | None,
) -> dict[str, int]:
    """ä¼°ç®—è¯·æ±‚ä½“è§„æ¨¡ï¼Œç”¨äºæ’æŸ¥è¯·æ±‚è¿‡å¤§é—®é¢˜"""
    system_chars = len(system_prompt)
    message_count = len(messages)
    message_chars = sum(len(str(msg.get("content", ""))) for msg in messages)
    tool_count = len(tools) if tools else 0
    tool_schema_chars = len(json.dumps(tools or [], ensure_ascii=False))

    payload_obj = {
        "model": "",
        "max_tokens": 0,
        "temperature": 0,
        "system": system_prompt,
        "messages": messages,
        "tools": tools or [],
    }
    payload_bytes = len(json.dumps(payload_obj, ensure_ascii=False).encode("utf-8"))

    return {
        "system_chars": system_chars,
        "message_count": message_count,
        "message_chars": message_chars,
        "tool_count": tool_count,
        "tool_schema_chars": tool_schema_chars,
        "payload_bytes": payload_bytes,
    }


class LLMService:
    """Anthropic Claude API å°è£…"""

    def __init__(
        self,
        api_key: str,
        model: str = "claude-sonnet-4-20250514",
        max_tokens: int = 8192,
        temperature: float = 0.0,
        base_url: str = "",
        timeout: float = 300.0,
        max_retries: int = 4,
    ) -> None:
        client_kwargs: dict[str, Any] = {
            "api_key": api_key,
            "timeout": timeout,
            "max_retries": 0,  # SDK å±‚ä¸é‡è¯•ï¼Œç”± _call_with_retry ç®¡ç†é‡è¯•å’Œæ—¥å¿—
        }
        if base_url:
            client_kwargs["base_url"] = base_url
        self._client = anthropic.Anthropic(**client_kwargs)
        self._model = model
        self._max_tokens = max_tokens
        self._temperature = temperature
        self._usage = TokenUsage()
        self._timeout = timeout
        # ä¿ç•™è¯¥å­—æ®µä»…ç”¨äºå…¼å®¹å†å²é…ç½®ï¼›_call_with_retry å·²é‡‡ç”¨æ— é™é‡è¯•ç­–ç•¥ã€‚
        self._max_retries = max_retries

    @property
    def usage(self) -> TokenUsage:
        return self._usage

    def call(
        self,
        system_prompt: str,
        messages: list[dict[str, str]],
        tools: list[dict[str, Any]] | None = None,
        conversation_log: ConversationLog | None = None,
        label: str = "",
    ) -> LLMResponse:
        """è°ƒç”¨ Claude API

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            conversation_log: å¯é€‰çš„å¯¹è¯æ—¥å¿—è®°å½•å™¨
            label: è°ƒç”¨æ ‡ç­¾ï¼Œç”¨äºæ—¥å¿—æ ‡è¯†ï¼ˆå¦‚ "Analyst/T0.1"ï¼‰

        Returns:
            LLMResponse åŒ…å«å†…å®¹ã€å·¥å…·è°ƒç”¨å’Œ token ç»Ÿè®¡
        """
        kwargs: dict[str, Any] = {
            "model": self._model,
            "max_tokens": self._max_tokens,
            "temperature": self._temperature,
            "system": system_prompt,
            "messages": messages,
        }
        if tools:
            kwargs["tools"] = tools

        # è¯·æ±‚å‰æ—¥å¿—ï¼šå¸®åŠ©å®šä½è¯·æ±‚ä½“è¿‡å¤§/å‚æ•°å¼‚å¸¸é—®é¢˜
        payload = _estimate_request_payload(system_prompt, messages, tools)
        tag = f"[{label}]" if label else "[LLM]"
        logger.info(
            f"    {tag} è¯·æ±‚ä½“ | msgs={payload['message_count']} | "
            f"msg_chars={payload['message_chars']} | system_chars={payload['system_chars']} | "
            f"tools={payload['tool_count']} | tool_chars={payload['tool_schema_chars']} | "
            f"payloadâ‰ˆ{payload['payload_bytes']}B"
        )

        # å¸¦é‡è¯•çš„ API è°ƒç”¨
        response = self._call_with_retry(label=label, **kwargs)

        # æå–æ–‡æœ¬å†…å®¹ï¼ˆè¿‡æ»¤ <think> æ ‡ç­¾ï¼‰
        text_parts: list[str] = []
        tool_calls: list[dict[str, Any]] = []
        for block in response.content:
            if block.type == "text":
                cleaned = _strip_think_tags(block.text)
                if cleaned:
                    text_parts.append(cleaned)
            elif block.type == "tool_use":
                tool_calls.append({
                    "id": block.id,
                    "name": block.name,
                    "input": block.input,
                })

        # æ›´æ–° token ç»Ÿè®¡
        input_tokens = response.usage.input_tokens
        output_tokens = response.usage.output_tokens
        self._usage.total_input += input_tokens
        self._usage.total_output += output_tokens

        result = LLMResponse(
            content="\n".join(text_parts),
            tool_calls=tool_calls,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=response.stop_reason or "",
        )

        # è®°å½•åˆ°å¯¹è¯æ—¥å¿—
        if conversation_log is not None:
            conversation_log.add_assistant(
                content=result.content,
                tool_calls=result.tool_calls or None,
            )
            conversation_log.add_token_usage(input_tokens, output_tokens)

        logger.info(
            f"    {tag} ç”¨é‡ | +{input_tokens} in / +{output_tokens} out | "
            f"ç´¯è®¡ calls={self._usage.total_calls}, in={self._usage.total_input}, out={self._usage.total_output}"
        )

        return result

    def _call_with_retry(self, label: str = "", **kwargs: Any) -> Any:
        """å¸¦é‡è¯•å’Œè¿›åº¦æ—¥å¿—çš„æµå¼ API è°ƒç”¨

        ä½¿ç”¨ streaming æ¨¡å¼ï¼Œè¶…æ—¶è®¡æ—¶å™¨ä¼šåœ¨æ¯æ¬¡æ”¶åˆ°æ•°æ®æ—¶é‡ç½®ï¼Œ
        åªæœ‰æœåŠ¡å™¨å®Œå…¨åœæ­¢å‘é€è¶…è¿‡ timeout ç§’æ‰ä¼šè¶…æ—¶ã€‚
        è¶…æ—¶æˆ–ä¸´æ—¶é”™è¯¯æ—¶è‡ªåŠ¨é‡è¯•ã€‚

        Args:
            label: æ—¥å¿—æ ‡ç­¾ï¼ˆå¦‚ "Analyst/T0.1"ï¼‰

        Returns:
            Anthropic API å“åº”å¯¹è±¡ (Message)

        Raises:
            anthropic.APIError: ä¸å¯é‡è¯•çš„ API é”™è¯¯
        """
        tag = f"[{label}]" if label else "[LLM]"
        attempt = 1
        retry_wait_seconds = 10
        max_retry_wait_seconds = 120

        while True:
            try:
                start = time.time()
                if attempt > 1:
                    logger.info(f"    {tag} é‡è¯• (ç¬¬ {attempt} æ¬¡)...")

                # æ˜¾ç¤ºç­‰å¾…æç¤º
                sys.stdout.write(f"\n    {tag} â³ ç­‰å¾…å“åº”...")
                sys.stdout.flush()

                # ä½¿ç”¨ streaming â€” å®æ—¶é€å­—è¾“å‡º LLM å›å¤åˆ°æ§åˆ¶å°
                with self._client.messages.stream(**kwargs) as stream:
                    streamed_text = False
                    for event in stream:
                        if hasattr(event, "type"):
                            if event.type == "content_block_delta":
                                delta = event.delta
                                if hasattr(delta, "text") and delta.text:
                                    if not streamed_text:
                                        # ç”¨ \r è¦†ç›–ç­‰å¾…æç¤º
                                        sys.stdout.write(f"\r    {tag} ")
                                        streamed_text = True
                                    sys.stdout.write(delta.text)
                                    sys.stdout.flush()
                    if streamed_text:
                        sys.stdout.write("\n")
                        sys.stdout.flush()
                    elif not streamed_text:
                        # çº¯å·¥å…·è°ƒç”¨æ— æ–‡æœ¬è¾“å‡ºæ—¶æ¸…é™¤ç­‰å¾…æç¤º
                        sys.stdout.write("\r" + " " * 60 + "\r")
                        sys.stdout.flush()
                    response = stream.get_final_message()

                elapsed = time.time() - start
                self._usage.total_calls += 1
                logger.debug(f"    {tag} å“åº” {elapsed:.1f}s (ç´¯è®¡ {self._usage.total_calls} æ¬¡)")
                return response

            except anthropic.APITimeoutError as e:
                elapsed = time.time() - start
                logger.warning(f"    {tag} è¶…æ—¶ ({elapsed:.0f}s) [ç¬¬ {attempt} æ¬¡]")
                logger.warning(f"    {tag} {retry_wait_seconds}s åé‡è¯•ï¼ˆé€€é¿ä¸Šé™ {max_retry_wait_seconds}sï¼‰")
                time.sleep(retry_wait_seconds)
                retry_wait_seconds = min(retry_wait_seconds * 2, max_retry_wait_seconds)
                attempt += 1
                continue

            except anthropic.APIConnectionError as e:
                logger.warning(f"    {tag} è¿æ¥é”™è¯¯ [ç¬¬ {attempt} æ¬¡]: {e}")
                logger.warning(f"    {tag} {retry_wait_seconds}s åé‡è¯•ï¼ˆé€€é¿ä¸Šé™ {max_retry_wait_seconds}sï¼‰")
                time.sleep(retry_wait_seconds)
                retry_wait_seconds = min(retry_wait_seconds * 2, max_retry_wait_seconds)
                attempt += 1
                continue

            except anthropic.RateLimitError as e:
                logger.warning(f"    {tag} é€Ÿç‡é™åˆ¶ [ç¬¬ {attempt} æ¬¡]: {e}")
                logger.warning(f"    {tag} {retry_wait_seconds}s åé‡è¯•ï¼ˆé€€é¿ä¸Šé™ {max_retry_wait_seconds}sï¼‰")
                time.sleep(retry_wait_seconds)
                retry_wait_seconds = min(retry_wait_seconds * 2, max_retry_wait_seconds)
                attempt += 1
                continue

            except anthropic.APIStatusError as e:
                # 5xx æœåŠ¡ç«¯é”™è¯¯å¯é‡è¯•ï¼Œ4xx ç›´æ¥æŠ›å‡º
                if e.status_code >= 500:
                    logger.warning(f"    {tag} æœåŠ¡ç«¯ {e.status_code} [ç¬¬ {attempt} æ¬¡]")
                    logger.warning(f"    {tag} 500è¯¦æƒ…: {_extract_api_status_error_detail(e)}")
                    logger.warning(f"    {tag} {retry_wait_seconds}s åé‡è¯•ï¼ˆé€€é¿ä¸Šé™ {max_retry_wait_seconds}sï¼‰")
                    time.sleep(retry_wait_seconds)
                    retry_wait_seconds = min(retry_wait_seconds * 2, max_retry_wait_seconds)
                    attempt += 1
                    continue
                else:
                    raise

            except Exception as e:
                if _is_retryable_timeout_error(e):
                    logger.warning(f"    {tag} åº•å±‚è¶…æ—¶ [ç¬¬ {attempt} æ¬¡]: {e}")
                    logger.warning(f"    {tag} {retry_wait_seconds}s åé‡è¯•ï¼ˆé€€é¿ä¸Šé™ {max_retry_wait_seconds}sï¼‰")
                    time.sleep(retry_wait_seconds)
                    retry_wait_seconds = min(retry_wait_seconds * 2, max_retry_wait_seconds)
                    attempt += 1
                    continue
                raise

    def call_with_tools_loop(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
        tools: list[dict[str, Any]],
        tool_executor: Any,
        max_iterations: int = 300,
        soft_limit: int = 30,
        conversation_log: ConversationLog | None = None,
        label: str = "",
    ) -> LLMResponse:
        """å¸¦å·¥å…·è°ƒç”¨å¾ªç¯çš„ LLM è°ƒç”¨

        æŒç»­è°ƒç”¨ç›´åˆ° LLM ä¸å†å‘èµ· tool_use æˆ–è¾¾åˆ°ç¡¬ä¸Šé™ã€‚
        è¾¾åˆ°è½¯é™åˆ¶æ—¶ï¼Œæ³¨å…¥åæ€æç¤ºè®© LLM è‡ªè¯„è¿›åº¦ï¼Œ
        è‹¥ LLM è®¤ä¸ºåº”ç»§ç»­åˆ™æ”¾è¡Œï¼Œå¦åˆ™ä¸­æ–­ã€‚

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰
            tool_executor: å·¥å…·æ‰§è¡Œå™¨ï¼Œéœ€è¦æœ‰ execute(name, input) -> str æ–¹æ³•
            max_iterations: ç¡¬ä¸Šé™è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ 300ï¼‰
            soft_limit: è½¯é™åˆ¶è½®æ¬¡ï¼Œè¾¾åˆ°æ—¶è§¦å‘åæ€æ£€æŸ¥ï¼ˆé»˜è®¤ 30ï¼‰
            conversation_log: å¯é€‰çš„å¯¹è¯æ—¥å¿—è®°å½•å™¨

        Returns:
            æœ€ç»ˆçš„ LLMResponse
        """
        current_messages = list(messages)
        final_response: LLMResponse | None = None
        last_real_response: LLMResponse | None = None  # æœ€åä¸€æ¬¡çœŸå® tool-loop å“åº”
        reflection_done = False
        tag = f"[{label}]" if label else "[LLM]"

        logger.info(f"    {tag} å¼€å§‹å·¥å…·å¾ªç¯ (ä¸Šé™ {max_iterations} è½®, è½¯é™åˆ¶ {soft_limit} è½®)")

        # åˆå§‹åŒ–å¯¹è¯æ—¥å¿—
        if conversation_log is not None:
            conversation_log.add_system(system_prompt)
            for msg in messages:
                if msg.get("role") == "user":
                    conversation_log.add_user(msg.get("content", ""))

        for iteration in range(max_iterations):
            # è½¯é™åˆ¶åæ€æ£€æŸ¥ï¼šè¾¾åˆ° soft_limit æ—¶æ³¨å…¥åæ€æç¤º
            if iteration == soft_limit and not reflection_done:
                reflection_done = True
                logger.warning(f"    {tag} å·²è¾¾è½¯é™åˆ¶ ({soft_limit} è½®)ï¼Œæ³¨å…¥åæ€æ£€æŸ¥...")
                reflection_prompt = (
                    f"[ç³»ç»Ÿæé†’] ä½ å·²ç»è¿›è¡Œäº† {soft_limit} è½®å·¥å…·è°ƒç”¨ã€‚è¯·ç®€çŸ­å›ç­”ï¼š\n"
                    f"1. å½“å‰ä»»åŠ¡è¿›å±•å¦‚ä½•ï¼ˆå·²å®Œæˆå“ªäº›æ–‡ä»¶/æ­¥éª¤ï¼‰ï¼Ÿ\n"
                    f"2. æ˜¯å¦å­˜åœ¨æ— æ•ˆå¾ªç¯ï¼ˆåå¤è¯»åŒä¸€æ–‡ä»¶ã€é‡å¤å¤±è´¥çš„æ“ä½œï¼‰ï¼Ÿ\n"
                    f"3. å‰©ä½™å·¥ä½œæ˜¯å¦å¯ä»¥åœ¨åˆç†è½®æ¬¡å†…å®Œæˆï¼Ÿ\n\n"
                    f"**é‡è¦ï¼šåªå›å¤ä»¥ä¸‹ä¸¤ç§ä¹‹ä¸€ï¼Œä¸è¦è¾“å‡ºæ–‡ä»¶å†…å®¹æˆ– JSONï¼š**\n"
                    f"- å›å¤ 'CONTINUE: <ä¸€å¥è¯è¯´æ˜å‰©ä½™è®¡åˆ’>' â€” ä»»åŠ¡æ­£åœ¨æ¨è¿›ï¼Œéœ€è¦ç»§ç»­ä½¿ç”¨å·¥å…·\n"
                    f"- å›å¤ 'DONE: <ä¸€å¥è¯è¯´æ˜å®Œæˆæƒ…å†µ>' â€” æ‰€æœ‰æ–‡ä»¶å·²é€šè¿‡å·¥å…·å†™å…¥ç£ç›˜ï¼Œæ— éœ€å†è°ƒç”¨å·¥å…·"
                )
                current_messages.append({"role": "user", "content": reflection_prompt})
                if conversation_log is not None:
                    conversation_log.add_user(reflection_prompt)

                # è°ƒç”¨ LLM è·å–åæ€ç»“æœï¼ˆä¸æä¾›å·¥å…·ï¼Œå¼ºåˆ¶çº¯æ–‡æœ¬å›å¤ï¼‰
                reflection_response = self.call(
                    system_prompt, current_messages, tools=None,
                    conversation_log=conversation_log,
                    label=f"{label}/åæ€" if label else "åæ€",
                )
                logger.info(f"    {tag} åæ€ç»“æœ: {reflection_response.content[:200]}")

                # å°†åæ€å›å¤åŠ å…¥æ¶ˆæ¯å†å²
                current_messages.append({
                    "role": "assistant",
                    "content": reflection_response.content,
                })

                if "CONTINUE" in reflection_response.content.upper():
                    logger.info(f"    {tag} LLM ç¡®è®¤ç»§ç»­ï¼Œæ”¾è¡Œè‡³ç¡¬ä¸Šé™")
                    continue
                else:
                    logger.info(f"    {tag} LLM å®Œæˆ (DONE)ï¼Œé€€å‡ºå·¥å…·å¾ªç¯")
                    # ä½¿ç”¨åæ€å‰æœ€åä¸€æ¬¡çœŸå® tool-loop å“åº”ï¼Œç¡®ä¿ from_json æœ‰å¯ç”¨å†…å®¹
                    if last_real_response is not None:
                        final_response = last_real_response
                    else:
                        final_response = reflection_response
                    break

            call_start = time.time()
            response = self.call(
                system_prompt, current_messages, tools,
                conversation_log=conversation_log,
                label=label,
            )
            call_elapsed = time.time() - call_start
            logger.info(
                f"    {tag} è½® {iteration + 1} | "
                f"tools={len(response.tool_calls)} | "
                f"+{response.input_tokens}in/+{response.output_tokens}out | "
                f"{call_elapsed:.1f}s"
            )
            final_response = response
            last_real_response = response  # è®°å½•æœ€åä¸€æ¬¡çœŸå® tool-loop å“åº”

            if not response.tool_calls:
                logger.info(f"    {tag} å®Œæˆ (å…± {iteration + 1} è½®)")
                break

            # æ„å»º assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_use blocksï¼‰
            assistant_content: list[dict[str, Any]] = []
            if response.content:
                assistant_content.append({"type": "text", "text": response.content})
            for tc in response.tool_calls:
                assistant_content.append({
                    "type": "tool_use",
                    "id": tc["id"],
                    "name": tc["name"],
                    "input": tc["input"],
                })
            current_messages.append({"role": "assistant", "content": assistant_content})

            # æ‰§è¡Œå·¥å…·å¹¶æ„å»º tool_result æ¶ˆæ¯
            tool_results: list[dict[str, Any]] = []
            for tc in response.tool_calls:
                tool_name = tc["name"]
                tool_input_summary = str(tc["input"])[:120]
                logger.info(f"    {tag} ğŸ”§ {tool_name}({tool_input_summary})")
                result = tool_executor.execute(tc["name"], tc["input"])
                result_str = str(result)
                logger.debug(f"    {tag} ğŸ”§ {tool_name} -> {result_str[:300]}")
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tc["id"],
                    "content": result_str,
                })
                # è®°å½•å·¥å…·ç»“æœ
                if conversation_log is not None:
                    conversation_log.add_tool_result(
                        tool_use_id=tc["id"],
                        tool_name=tc["name"],
                        result=result_str,
                    )
            current_messages.append({"role": "user", "content": tool_results})

        else:
            # for å¾ªç¯æ­£å¸¸ç»“æŸ = è¾¾åˆ°ç¡¬ä¸Šé™
            logger.error(f"    {tag} è¾¾åˆ°ç¡¬ä¸Šé™ ({max_iterations} è½®)ï¼Œå¼ºåˆ¶ä¸­æ–­ï¼")

        logger.info(
            f"    {tag} å¾ªç¯ç»“æŸ | ç´¯è®¡ {self._usage.total_input}in/{self._usage.total_output}out"
        )

        assert final_response is not None
        return final_response
